
@misc{rossi_early_2024,
	title = {An {Early} {Categorization} of {Prompt} {Injection} {Attacks} on {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.00898},
	doi = {10.48550/arXiv.2402.00898},
	abstract = {Large language models and AI chatbots have been at the forefront of democratizing artificial intelligence. However, the releases of ChatGPT and other similar tools have been followed by an increasing number of concerns regarding the difficulty of controlling large language models and their outputs. Currently, we are witnessing a cat-and-mouse game where users attempt to misuse the models with a novel attack called prompt injections while the developers attempt to discover the vulnerabilities and block the attacks simultaneously. In this paper, we provide an overview of this emergent threat and present a categorization of prompt injections, which can guide future research on prompt injections and act as a checklist of vulnerabilities in the development of LLM interfaces. Moreover, based on previous literature and our own empirical research, we discuss the implications of prompt injections to LLM end users, developers, and researchers.},
	language = {en},
	urldate = {2025-06-16},
	publisher = {arXiv},
	author = {Rossi, Sippo and Michel, Alisia Marianne and Mukkamala, Raghava Rao and Thatcher, Jason Bennett},
	month = jan,
	year = {2024},
	note = {arXiv:2402.00898 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	annote = {Comment: 21 pages double spacing},
	file = {PDF:/home/landro/Zotero/storage/R94EAGLL/Rossi et al. - 2024 - An Early Categorization of Prompt Injection Attacks on Large Language Models.pdf:application/pdf},
}

@misc{liu_automatic_2024,
	title = {Automatic and {Universal} {Prompt} {Injection} {Attacks} against {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2403.04957},
	doi = {10.48550/arXiv.2403.04957},
	abstract = {Large Language Models (LLMs) excel in processing and generating human language, powered by their ability to interpret and follow instructions. However, their capabilities can be exploited through prompt injection attacks. These attacks manipulate LLM-integrated applications into producing responses aligned with the attacker's injected content, deviating from the user's actual requests. The substantial risks posed by these attacks underscore the need for a thorough understanding of the threats. Yet, research in this area faces challenges due to the lack of a unified goal for such attacks and their reliance on manually crafted prompts, complicating comprehensive assessments of prompt injection robustness. We introduce a unified framework for understanding the objectives of prompt injection attacks and present an automated gradient-based method for generating highly effective and universal prompt injection data, even in the face of defensive measures. With only five training samples (0.3\% relative to the test data), our attack can achieve superior performance compared with baselines. Our findings emphasize the importance of gradient-based testing, which can avoid overestimation of robustness, especially for defense mechanisms.},
	language = {en},
	urldate = {2025-06-16},
	publisher = {arXiv},
	author = {Liu, Xiaogeng and Yu, Zhiyuan and Zhang, Yizhe and Zhang, Ning and Xiao, Chaowei},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04957 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Pre-print, code is available at https://github.com/SheltonLiu-N/Universal-Prompt-Injection},
	file = {PDF:/home/landro/Zotero/storage/A33TSAN5/Liu et al. - 2024 - Automatic and Universal Prompt Injection Attacks against Large Language Models.pdf:application/pdf},
}

@article{kwon_text-based_2024,
	title = {Text-{Based} {Prompt} {Injection} {Attack} {Using} {Mathematical} {Functions} in {Modern} {Large} {Language} {Models}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/13/24/5008},
	doi = {10.3390/electronics13245008},
	abstract = {Prompt injection is a type of attack that induces violent or discriminatory responses via the input of a prompt containing illegal instructions to the large language model (LLM). Most early injection attacks used simple text prompts; however, recently, injection attacks employing elaborately designed prompts to overcome the strong security policies of modern LLMs have been applied to input prompts. This study proposed a method to perform injection attacks that can bypass existing security policies via the replacement of sensitive words that may be rejected by a language model in the text prompt with mathematical functions. By hiding the contents of the prompt so that the LLM cannot easily detect the contents of the illegal instructions, we achieved a considerably higher success rate than existing injection attacks, even for the latest securely aligned LLMs. As the proposed method employed only text prompts, it was capable of attacking most LLMs. Moreover, it exhibited a higher attack success rate than multimodal attacks using images despite using only text. An understanding of the newly proposed injection attack is expected to aid in the development of methods to further strengthen the security of current LLMs.},
	language = {en},
	number = {24},
	urldate = {2025-06-16},
	journal = {Electronics},
	author = {Kwon, Hyeokjin and Pak, Wooguil},
	month = jan,
	year = {2024},
	note = {Number: 24
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {large language model, mask, mathematical function, prompt injection},
	pages = {5008},
	file = {Full Text PDF:/home/landro/Zotero/storage/U8CKHBXX/Kwon and Pak - 2024 - Text-Based Prompt Injection Attack Using Mathematical Functions in Modern Large Language Models.pdf:application/pdf},
}

@article{lee_mind_2025,
	title = {Mind {Mapping} {Prompt} {Injection}: {Visual} {Prompt} {Injection} {Attacks} in {Modern} {Large} {Language} {Models}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Mind {Mapping} {Prompt} {Injection}},
	url = {https://www.mdpi.com/2079-9292/14/10/1907},
	doi = {10.3390/electronics14101907},
	abstract = {Large language models (LLMs) have made significant strides in generating coherent and contextually relevant responses across diverse domains. However, these advancements have also led to an increase in adversarial attacks, such as prompt injection, where attackers embed malicious instructions within prompts to bypass security filters and manipulate LLM outputs. Various injection techniques, including masking and encoding sensitive words, have been employed to circumvent security measures. While LLMs continuously enhance their security protocols, they remain vulnerable, particularly in multimodal contexts. This study introduces a novel method for bypassing LLM security policies by embedding malicious instructions within a mind map image. The attack leverages the intentional incompleteness of the mind map structure, specifically the absence of explanatory details. When the LLM processes the image and fills in the missing sections, it inadvertently generates unauthorized outputs, violating its intended security constraints. This approach applies to any LLM capable of extracting and interpreting text from images. Compared to the best-performing baseline method, which achieved an ASR of 30.5\%, our method reaches an ASR of 90\%, yielding an approximately threefold-higher attack success. Understanding this vulnerability is crucial for strengthening security policies in state-of-the-art LLMs.},
	language = {en},
	number = {10},
	urldate = {2025-06-16},
	journal = {Electronics},
	author = {Lee, Seyong and Kim, Jaebeom and Pak, Wooguil},
	month = jan,
	year = {2025},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {large language model, prompt injection, mind map, multimodal},
	pages = {1907},
	file = {Full Text PDF:/home/landro/Zotero/storage/CSAXF2EY/Lee et al. - 2025 - Mind Mapping Prompt Injection Visual Prompt Injection Attacks in Modern Large Language Models.pdf:application/pdf},
}

@article{clusmann_prompt_2025,
	title = {Prompt injection attacks on vision language models in oncology},
	volume = {16},
	copyright = {2025 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-55631-x},
	doi = {10.1038/s41467-024-55631-x},
	abstract = {Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be compromised by prompt injection attacks. These can be used to output harmful information just by interacting with the VLM, without any access to its parameters. We perform a quantitative study to evaluate the vulnerabilities to these attacks in four state of the art VLMs: Claude-3 Opus, Claude-3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N = 594 attacks, we show that all of these models are susceptible. Specifically, we show that embedding sub-visual prompts in manifold medical imaging data can cause the model to provide harmful output, and that these prompts are non-obvious to human observers. Thus, our study demonstrates a key vulnerability in medical VLMs which should be mitigated before widespread clinical adoption.},
	language = {en},
	number = {1},
	urldate = {2025-06-16},
	journal = {Nature Communications},
	author = {Clusmann, Jan and Ferber, Dyke and Wiest, Isabella C. and Schneider, Carolin V. and Brinker, Titus J. and Foersch, Sebastian and Truhn, Daniel and Kather, Jakob Nikolas},
	month = feb,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer imaging, Computational science, Machine learning, Medical imaging},
	pages = {1239},
	file = {Full Text PDF:/home/landro/Zotero/storage/9ZJZLC9C/Clusmann et al. - 2025 - Prompt injection attacks on vision language models in oncology.pdf:application/pdf},
}

@article{mathew_enhancing_nodate,
	title = {Enhancing {Security} in {Large} {Language} {Models}: {A} {Comprehensive} {Review} of {Prompt} {Injection} {Attacks} and {Defenses}},
	shorttitle = {Enhancing {Security} in {Large} {Language} {Models}},
	url = {https://www.authorea.com/users/838696/articles/1229733-enhancing-security-in-large-language-models-a-comprehensive-review-of-prompt-injection-attacks-and-defenses},
	abstract = {This review paper explores advanced methods to prompt Large Language Models (LLMs) into generating objectionable or unintended behaviors through adversarial prompt injection attacks. We examine a series of novel projects like HOUYI, Robustly Aligned L},
	urldate = {2025-06-16},
	author = {Mathew, Eleena},
	file = {Full Text PDF:/home/landro/Zotero/storage/3RAN5ZJU/Mathew - Enhancing Security in Large Language Models A Comprehensive Review of Prompt Injection Attacks and.pdf:application/pdf},
}

@misc{noauthor_prompt_2025,
	title = {Prompt injection},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Prompt_injection&oldid=1289403591},
	abstract = {Prompt injection is a cybersecurity exploit in which adversaries craft inputs that appear legitimate but are designed to cause unintended behavior in  machine learning models, particularly large language models (LLMs). This attack takes advantage of the model's inability to distinguish between developer-defined prompts and user inputs, allowing adversaries to bypass safeguards and influence model behaviour. While LLMs are designed to follow trusted instructions, they can be manipulated into carrying out unintended responses through carefully crafted inputs.
The Open Worldwide Application Security Project (OWASP) ranked prompt injection as the top security risk in its 2025 OWASP Top 10 for LLM Applications report, describing it as a vulnerability that can manipulate LLMs through adversarial inputs.},
	language = {en},
	urldate = {2025-06-16},
	journal = {Wikipedia},
	month = may,
	year = {2025},
	note = {Page Version ID: 1289403591},
	file = {Snapshot:/home/landro/Zotero/storage/8IGNC95Y/Prompt_injection.html:text/html},
}

@misc{willison_simon_nodate,
	title = {Simon {Willison}’s {Weblog}},
	url = {https://simonwillison.net/},
	language = {en-gb},
	urldate = {2025-06-17},
	journal = {Simon Willison’s Weblog},
	author = {Willison, Simon},
	file = {Snapshot:/home/landro/Zotero/storage/KG3VUK8H/simonwillison.net.html:text/html},
}

@inproceedings{fasha_mitigating_2024,
	title = {Mitigating the {OWASP} {Top} 10 {For} {Large} {Language} {Models} {Applications} using {Intelligent} {Agents}},
	url = {https://ieeexplore.ieee.org/abstract/document/10532874},
	doi = {10.1109/ICCR61006.2024.10532874},
	abstract = {Large Language Models (LLMs) have emerged as a transformative and disruptive technology, enabling a wide range of applications in natural language processing, machine translation, and beyond. However, this widespread integration of LLMs also raised several security concerns highlighted by the Open Web Application Security Project (OWASP), which has identified the top 10 security vulnerabilities inherent in LLM applications. Addressing these vulnerabilities is crucial, given the increasing reliance on LLMs and the potential threats to data integrity, confidentiality, and service availability. This paper presents a framework designed to mitigate the security risks outlined in the OWASP Top 10. Our proposed model leverages LLM-enabled intelligent agents, offering a new approach to proactively identify, assess, and counteract security threats in real-time. The proposed framework serves as an initial blueprint for future research and development, aiming to enhance the security measures of LLMs and protect against emerging threats in this rapidly evolving landscape.},
	urldate = {2025-06-17},
	booktitle = {2024 2nd {International} {Conference} on {Cyber} {Resilience} ({ICCR})},
	author = {Fasha, Mohammad and Rub, Faisal Abul and Matar, Nasim and Sowan, Bilal and Al Khaldy, Mohammad and Barham, Hussam},
	month = feb,
	year = {2024},
	keywords = {AutoGen, Intelligent agents, Large Language Model (LLM), Machine translation, OWASP Top 10, Proposals, Protection, Real-time systems, Retrieval Augmented Generation (RAG), Security, Technological innovation},
	pages = {1--9},
	file = {Snapshot:/home/landro/Zotero/storage/DYU6SZER/10532874.html:text/html},
}

@article{willison_prompt_nodate,
	title = {Prompt injection: {What}’s the worst that can happen? {Apr}. 14, 2023},
	journal = {URL: https://perma. cc/D7B6-ESAX},
	author = {Willison, Simon},
}

@misc{noauthor_securing_2023,
	title = {Securing {LLM} {Systems} {Against} {Prompt} {Injection}},
	url = {https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/},
	abstract = {This post explains prompt injection and shows how the NVIDIA AI Red Team identified vulnerabilities where prompt injection can be used to exploit three plug-ins included in the LangChain library.},
	language = {en-US},
	urldate = {2025-06-17},
	journal = {NVIDIA Technical Blog},
	month = aug,
	year = {2023},
	file = {Snapshot:/home/landro/Zotero/storage/IFANT8ZB/securing-llm-systems-against-prompt-injection.html:text/html},
}

@misc{noauthor_quest-ce_nodate,
	title = {Qu'est-ce qu'une attaque par injection d'invite ? - {Logiciel} {Check} {Point}},
	shorttitle = {Qu'est-ce qu'une attaque par injection d'invite ?},
	url = {https://www.checkpoint.com/fr/cyber-hub/cyber-security/what-is-cyber-attack/what-is-a-prompt-injection-attack/},
	abstract = {Découvrez ce qu'est une attaque par injection rapide, les différents types d'attaques observés dans la nature et les meilleures pratiques de prévention.},
	language = {fr-FR},
	urldate = {2025-06-21},
	journal = {Check Point Software},
	file = {Snapshot:/home/landro/Zotero/storage/H95CEAQW/what-is-a-prompt-injection-attack.html:text/html},
}

@article{burgess_security_nodate,
	title = {The {Security} {Hole} at the {Heart} of {ChatGPT} and {Bing}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/chatgpt-prompt-injection-attack-security/},
	abstract = {Indirect prompt-injection attacks can leave people vulnerable to scams and data theft when they use the AI chatbots.},
	language = {en-US},
	urldate = {2025-06-21},
	journal = {Wired},
	author = {Burgess, Matt},
	note = {Section: tags},
	keywords = {artificial intelligence, bing, chatgpt, data, privacy},
	file = {Snapshot:/home/landro/Zotero/storage/FXM5T35G/chatgpt-prompt-injection-attack-security.html:text/html},
}

@misc{zhang_defense_2025,
	title = {Defense against {Prompt} {Injection} {Attacks} via {Mixture} of {Encodings}},
	url = {http://arxiv.org/abs/2504.07467},
	doi = {10.48550/arXiv.2504.07467},
	abstract = {Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics.},
	urldate = {2025-06-22},
	publisher = {arXiv},
	author = {Zhang, Ruiyi and Sullivan, David and Jackson, Kyle and Xie, Pengtao and Chen, Mei},
	month = apr,
	year = {2025},
	note = {arXiv:2504.07467 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/landro/Zotero/storage/8Z73YUPM/Zhang et al. - 2025 - Defense against Prompt Injection Attacks via Mixture of Encodings.pdf:application/pdf;Snapshot:/home/landro/Zotero/storage/JV6F39Y7/2504.html:text/html},
}

@misc{zhang_formalizing_2025,
	title = {Formalizing and {Benchmarking} {Prompt} {Injection} {Attacks} and {Defenses}},
	url = {https://gnehcuyz.github.io/reading/prompt-injection-attacks/},
	abstract = {Analyzes prompt injection attacks in LLMs, evaluates their impact on different models, and benchmarks defenses like Known-Answer Detection.},
	language = {en},
	urldate = {2025-06-22},
	journal = {Chengyu Zhang},
	author = {Zhang, Chengyu},
	month = feb,
	year = {2025},
	note = {Section: reading},
	file = {Snapshot:/home/landro/Zotero/storage/CVAA7WVY/prompt-injection-attacks.html:text/html},
}

@misc{chen_secalign_2025,
	title = {{SecAlign}: {Defending} {Against} {Prompt} {Injection} with {Preference} {Optimization}},
	shorttitle = {{SecAlign}},
	url = {http://arxiv.org/abs/2410.05451},
	doi = {10.48550/arXiv.2410.05451},
	abstract = {Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to around 0\%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, our defended models are still practical with similar utility to the one before our defensive training. Our code is at https://github.com/facebookresearch/SecAlign},
	urldate = {2025-06-22},
	publisher = {arXiv},
	author = {Chen, Sizhe and Zharmagambetov, Arman and Mahloujifar, Saeed and Chaudhuri, Kamalika and Wagner, David and Guo, Chuan},
	month = jan,
	year = {2025},
	note = {arXiv:2410.05451 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	annote = {Comment: Key words: prompt injection defense, LLM security, LLM-integrated applications},
	file = {Preprint PDF:/home/landro/Zotero/storage/XUM9N7IE/Chen et al. - 2025 - SecAlign Defending Against Prompt Injection with Preference Optimization.pdf:application/pdf;Snapshot:/home/landro/Zotero/storage/6MSG466G/2410.html:text/html},
}

@misc{zhou_spin_2024,
	title = {{SPIN}: {Self}-{Supervised} {Prompt} {INjection}},
	shorttitle = {{SPIN}},
	url = {http://arxiv.org/abs/2410.13236},
	doi = {10.48550/arXiv.2410.13236},
	abstract = {Large Language Models (LLMs) are increasingly used in a variety of important applications, yet their safety and reliability remain as major concerns. Various adversarial and jailbreak attacks have been proposed to bypass the safety alignment and cause the model to produce harmful responses. We introduce Self-supervised Prompt INjection (SPIN) which can detect and reverse these various attacks on LLMs. As our self-supervised prompt defense is done at inference-time, it is also compatible with existing alignment and adds an additional layer of safety for defense. Our benchmarks demonstrate that our system can reduce the attack success rate by up to 87.9\%, while maintaining the performance on benign user requests. In addition, we discuss the situation of an adaptive attacker and show that our method is still resilient against attackers who are aware of our defense.},
	urldate = {2025-06-22},
	publisher = {arXiv},
	author = {Zhou, Leon and Yang, Junfeng and Mao, Chengzhi},
	month = oct,
	year = {2024},
	note = {arXiv:2410.13236 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/landro/Zotero/storage/7JYE4QCS/Zhou et al. - 2024 - SPIN Self-Supervised Prompt INjection.pdf:application/pdf;Snapshot:/home/landro/Zotero/storage/YMW5HZWC/2410.html:text/html},
}
